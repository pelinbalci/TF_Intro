{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lecture_7_tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pelinbalci/TF_Intro/blob/main/lecture_7_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8jdqR9NNb5a"
      },
      "source": [
        "Ref: https://www.youtube.com/watch?v=f5YJA5mQD5c&ab_channel=GoogleDevelopers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVa45EGIXO0H"
      },
      "source": [
        "Common coding format is called ASCII.\n",
        "\n",
        "Common letters and symbols are encoded into values from 0 - 255\n",
        "\n",
        "It is useful in that 1 byte is needed to store the value for a letter. It has to be superseded by later encodings i.o to give access to characters and letters beyond 255. (in partivular international chars)\n",
        "\n",
        "\n",
        "LISTEN : 076, 073, 083 084, 069, 078  -> 6 bytes\n",
        "\n",
        "Here we will learn word-based encoding not letter-based.\n",
        "\n",
        "LISTEN and SILENT have same letters and opposite meanings :)\n",
        "\n",
        "A computer doesn't tell the difference btw these two words with letterbased ancoding unless we have a sequence model.( it is a bit complicated)\n",
        "\n",
        "\n",
        "**Word-Based Encoding**\n",
        "\n",
        "Each word respresented by a single number. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYbOdGLbXE_-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%201%20-%20Lesson%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BZSlp3DAjdYf"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaCMcjMQifQc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e8d647-1ccf-4fba-8c65-188802190d48"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "# num words precifies the maximum number of words that you want to care about. \n",
        "# assign tokens to words based on how commonly used they are in corpus. \n",
        "# most common word will be at index 1. \n",
        "# ! is automtically removed:)\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100) \n",
        "tokenizer.fit_on_texts(sentences) \n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YEKJ6DJjz6L",
        "outputId": "047a2797-9cd5-4eb1-809c-14239fcaf711"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'merhaba, benim adım pelin',\n",
        "    'benim kedim var',\n",
        "    'sen neredesin?',\n",
        "    'sen buraya gel.',\n",
        "    'benim evim nerede',\n",
        "    'kedim',\n",
        "    'adım xxx',\n",
        "    'Sen',\n",
        "    'sen',\n",
        "    'Sen',\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sen': 1, 'benim': 2, 'adım': 3, 'kedim': 4, 'merhaba': 5, 'pelin': 6, 'var': 7, 'neredesin': 8, 'buraya': 9, 'gel': 10, 'evim': 11, 'nerede': 12, 'xxx': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}